---
title: "**Machine Learning - Human Activity Classifier**"
author: "FNwobu"
date: "05/08/2021"
geometry: margin=3cm
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE, eval = TRUE}
library(ggplot2);library(dplyr);library(reshape2);library(caret)
library(xgboost);library(ggpubr);library(data.table)
knitr::opts_chunk$set(echo = FALSE, eval = FALSE)
```

## **Executive Summary**

In this study, we investigate accelerometer data collected from 6 participants taking part in an human activity experiment. The aim is to predict which activity a participant is doing based on data from accelerometers attached to their belt, forearm, arm and dumbell. In recent years, the use of activity devices such as Jawbone Up, Nike FuelBand Fitbit has increased along with people's interest in being able to quantify how much and how well they do particular activities. As such, there is a demand for algorithms that are able to make sense of this ever increasing mass of activity data being collected. 

The data analysed in this project comes from accelerometer data attached to participants asked to perform barbell lifts correctly and incorrectly in 5 different ways. As such, classification models were selected in order to generate predictions of a given participant's activity. eXtreme Gradient Boost (XGBoost), Random Forest and k-Nearest Neighbours were considered in the analysis. Two datasets were provided - a training set (19,622 observations) and a hold-out set (20 observations) - of which, the training set was further subset (into training, test and validation) in order to build the three classification models. The analysis proceeded as follows; data pre-processing (ensuring consistent data quality between training and hold-out sets, feature selection), model building (hyperparameter tuning for both XGBoost, random forest and k-Nearest Neighbour models), model ensembling (majority vote and stacking) and finally, model evaluation on the validation dataset. 

It was determined that the k-Nearest Neighbour model had the highest estimated accuracy on the validation dataset (possibly due to computational constraints limiting the range of number trees investigated for the XGBoost and Random Forest models). However, a model stack (of all models) was selected as the final model - accuracy at 95% level confidence intervals = (0.983,0.991) - to ensure robustness against noise, outliers, data mislabelling etc.

## **Sections**

1. **Data Pre-Processing**
    a. Exploratory Analysis
    b. Feature Selection
    c. Train/Test/Validation Split
    d. Distribution Checks
    e. Feature Processing
2. **Model Training**
    a. XGBoost Hyperparameter Tuning
    b. Random Forest Hyperparameter Tuning
    c. k-Nearest Neighbours Hyperparameter Tuning
3. **Model Ensembling**
    a. Majority Voting
    b. Model Stacking
4. **Model Evaluation**

# **Method**

```{r load myData, include=FALSE, eval = TRUE, cache = TRUE}
#Load data
load("model_data/final_model_data.RData")
```

## **Data Pre-Processing**

```{r load_data, echo=FALSE}
# ---------
# Load Data - training and hold-out sample
# ---------
training_raw <- read.csv("data/pml-training.csv")
#load hold-out sample data
holdout_raw <- read.csv("data/pml-testing.csv")
```

### Exploratory Analysis

Initial inspection of the training dataset confirms the existence of five class labels; `r unique(training_raw$classe)`,  across `r dim(training_raw)[1]` observations described by `r dim(training_raw)[2] - 1` features.

### Feature Selection
```{r preproc, echo=FALSE}
# ---------
# Format conversion - convert classification variable to factor
# ---------
training_raw$classe <- as.factor(training_raw$classe)

# ---------
# Remove irrelevant factors - make independent of participant and time
# ---------
# X: row number
# user_name: Model predictions should be independent of the actual participant
# raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp: predictions should be independent of time
# new_window, num_window: not relevant movement of study participant
facs_to_rmv <- c("X","user_name","raw_timestamp_part_1","raw_timestamp_part_2",
                 "cvtd_timestamp","new_window","num_window")
training_raw2 <- training_raw %>% select(-one_of(facs_to_rmv))
holdout_raw2 <- holdout_raw %>% select(-one_of(facs_to_rmv))

# ---------
# Remove variables w/ NAs in holdout dataset
# ---------
#extract the variable names from train/holdout datasets
vars_train <- names(training_raw2)
vars_holdout <- names(holdout_raw2)
# reconcile the columns between train and holdout data - 
# classe in only in train, problem_id only in holdout
mismatch <- data.frame(in_train_out_holdout = setdiff(vars_train,vars_holdout),
                       out_train_in_holdout = setdiff(vars_holdout,vars_train))
#identify variables in holdout dataset w/ NAs
vars_to_rmv <- vars_holdout[Reduce(rbind, lapply(lapply(holdout_raw2, is.na),any))]
#subset train/holdout data to non-NA variables
training_tmp <- training_raw2 %>% select(-one_of(vars_to_rmv))
holdout <- holdout_raw2 %>% select(-one_of(vars_to_rmv))
# Training dataset reduced to 53 variables including the classifier (classe)
dim(training_tmp)

# ---------
# Subset data in train/test - 70/15/15 split for Train/Test/Validation
# ---------
set.seed(123)
inTrain  <- createDataPartition(training_tmp$classe, p = 0.7, list = FALSE)
training <- training_tmp[inTrain,]
testing_tmp  <- training_tmp[-inTrain,]
set.seed(123)
inTest  <- createDataPartition(testing_tmp$classe, p = 0.5, list = FALSE)
testing <- testing_tmp[inTest,]
validation  <- testing_tmp[-inTest,]
```
The following features were removed from the dataset:

* "X" : Observation row number - not predictive
* "user_name" : Model predictions should not be specific to the participant
* "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp": Model predictions should be independent of time
* "new_window", "num_window" : Unrelated to participant movement

Additionally, NA values were found in the columns of `r length(vars_to_rmv)` features in the holdout sample. As such, these features were also removed for the training dataset. This reduced the overall number of predictive features to `r dim(training_tmp)[2]-1`.

### Train/Test/Validation Data Split

In this study, the initial "training" dataset was split 70/15/15 into the train, test and validation sets. The train dataset was used to train and tune the hyperparameters of each model (via cross-validation). The test dataset was used to create a model ensemble of the final XGB, Random Forest and K-NN models. The validation set was used for model evaluation and thus to estimate the accuracy of each model on the holdout data sample.

### Distribution Checks

As the task presented in this study is a multinomial classification problem, prior to model training it was necessary to check whether labels in the input data were evenly distributed between each class. If not treated appropriately, an imbalanced class distribution can lead to a model that has poor predictive performance for the minority class as the model will be biased towards optimising for high accuracy on the majority class. In the human activity we see a relatively even class distribution (see **Table 1** below). The percentage of each class in the train, test and validation datasets varies between 16-29%.
```{r strat_check, echo=FALSE}
# ---------
# Distribution checks - slight bias towards "A" class
# ---------
train_classe <- training %>% group_by(classe) %>% 
        dplyr::summarise(train_n = n()) %>%
        dplyr::mutate(train_pct = round(100 * train_n / sum(train_n,na.rm=TRUE),1))
test_classe <- testing %>% group_by(classe) %>% 
        dplyr::summarise(test_n = n()) %>% 
        dplyr::mutate(test_pct = round(100 * test_n / sum(test_n,na.rm=TRUE),1)) %>% 
                select(-classe)
validation_classe <- validation %>% group_by(classe) %>% 
        dplyr::summarise(validation_n = n()) %>%
        dplyr::mutate(validation_pct = round(100 * validation_n / sum(validation_n,na.rm=TRUE),1)) %>% 
                select(-classe)
```

```{r strat_check_output, echo=FALSE, eval = TRUE, cache = TRUE}
knitr::kable(cbind(train_classe,test_classe,validation_classe), caption = "**Table 1**: Number/Percentage of each class in the training, test and validation datasets")
# Skewed towards more of class A in all three datasets - overall proportion of classes
# consistent across train, test and validation datasets
```


### Feature processing - Principal Component Analysis (PCA)
Principal Component Analysis (PCA) was used in order to reduce the number of features modelled and thus save on computation time. A 95% cut-off for the explained variance was applied. This corresponded to the first 25 principal components (in order of decreasing explained variance).

```{r apply_pca, echo=FALSE}
# -------------
# PCA - feature reduction; take principal components that capture 95% of explained variance
# -------------
# use prcomp to obtain variances of principal components
pca_comp <- data.frame(var = (prcomp(training[,-53], center = TRUE, scale = TRUE)$sdev)^2) %>%
        dplyr::mutate(PC = row_number(),  var_pct = 100 *var/sum(var, na.rm = TRUE),
               var_pct_cum = cumsum(var_pct))

preproc <- preProcess(training[,-53],thresh = 0.95,method="pca")
preproc
trainpca <- predict(preproc, training[,-53])
trainpca$classe <- training$classe
testpca <- predict(preproc, testing[,-53])
testpca$classe <- testing$classe
validationpca <- predict(preproc, validation[,-53])
validationpca$classe <- validation$classe

#Convert input data into Dmatrices for XGB model
X_train = xgb.DMatrix(as.matrix(trainpca %>% select(-classe)))
y_train = trainpca$classe
X_test = xgb.DMatrix(as.matrix(testpca %>% select(-classe)))
y_test = testpca$classe
X_validation = xgb.DMatrix(as.matrix(validationpca %>% select(-classe)))
y_validation = validationpca$classe
```

```{r plot_pca, echo=FALSE, eval = TRUE, cache = TRUE, fig.align='center', fig.height=4, fig.width=8, fig.cap="**Figure 1**: Percentage of variance explained by each principal component (52 in total). Dashed blue line indicates the 95% cut-off at the 25th principal component."}
plot_pca <- ggplot(data = pca_comp) + 
        geom_bar( aes(x = PC, y = var_pct), stat = "identity", fill = "red", alpha = 0.6) +
        labs(x="Principal Component", y = " % of Variance Explained") +
        scale_x_continuous(expand = c(0, 0), breaks = c(1,seq(5,50,5)))+ 
        scale_y_continuous(limits = c(0,17), expand = c(0, 0))  +      
        geom_vline(xintercept = 25, linetype="dashed", color="blue") +
        theme_bw() +
        theme(axis.line = element_line(colour = "black"),
              panel.grid.major = element_blank(),
              panel.grid.minor = element_blank(),
              panel.background = element_blank())
plot_pca
```

## **Model Training**

Three models were trained in this study:

* Xtreme Gradient Boost (XGBoost)
* Random Forest
* k-Nearest Neighbours

To optimise the models, the constituent hyperparamers were tuned on the training data using cross validation on 10-folds repeated 3 times. The models were tuned using the in-built tuning functionality in the caret "train" function (excl. Random Forest - see section below). The accuracy of each model was then computed using a 95% confidence interval assuming a student-t distribution of the cross validated model results. Distribution checks on the training data showed there was a reasonably even distribution of the five classes, as such, accuracy was deemed an appropriate evaluation metric for the trained models.

```{r train_control, echo=FALSE}
# -------------
# Set train parameters for caret - cross validation & no. of repeats
# -------------
# Repeated cross-fold validation (repeatedcv) on 10 folds (number) and repeated 
# 3 times (repeats)
kfolds <- 10; cv_repeats <- 3; nctrl <- kfolds * cv_repeats
zctrl_95 <- qt(0.975, kfolds * cv_repeats - 1)
ctrl <- trainControl(method = "repeatedcv", number = kfolds, repeats = cv_repeats, 
                     allowParallel = TRUE,verboseIter = FALSE, 
                     savePredictions = TRUE, classProbs = TRUE,
                     returnData = FALSE)
```

### Xtreme Gradient Boost (XGBoost) Tuning

XGBoost models is an ensemble method in which weak learners (boosted trees) are fit sequentially to the residuals of previous models. Ensemble methods have a high predictive accuracy but can also be complex and are prone to overfitting. In XGBoost models overfitting is primarily controlled by reducing the complexity of each weak learner via regularisation, pruning (removes tree splits before/after build process), sampling (reduces inter-tree correlation) and early stopping (fix number of trees).

In this study, the "xgbTree" XGBoost method is used in the caret train function. In this method, the following hyperparameter grid is tuned:

```{r hp_grid_xgb, echo=FALSE}
# set out tuning hyperparameter grid
xgb_hypergrid <- expand.grid(nrounds = c(50,100,175,250),
                             max_depth = c(5,8,12),
                             eta = 0.1,
                             gamma = 0,
                             colsample_bytree = c(0.5,0.75,1),
                             subsample = 1,
                             min_child_weight = 0)

```

* **eta** = `r unique(xgb_hypergrid$eta)`
  * Learning rate: multiply each tree output value by a number to make the model fit slower - the lower the learning rate, the more robust the model is to preventing overfitting. 
  * Ideally, a lower number would be used in the grid would be used but computational constraints restrict number of trees in model

* **gamma** = `r unique(xgb_hypergrid$gamma)`
  * Minimum gain improvement for a leaf (split) to be retained in the tree. The higher the number, the more conservative (i.e. less complex) the model and less prone to overfitting. 
  * Held constant as tuning this parameter properly would require detailed analysis of gain distribution across nodes 

* **min_child_weight** = `r unique(xgb_hypergrid$min_child_weight)`
  * Minimum sum of Hessians (second derivatives) needed to keep a child node during a partitioning (a split). A higher value leads to a more conservative model. 
  * Held fixed due to tuning complexity 

* **max_depth** = `r unique(xgb_hypergrid$max_depth)` 
  * Maximum depth of a tree (i.e. number of splits). Acts as a hard stop on the tree build process as deeper trees are prone to overfitting. Too low a value leads to high bias as feature interaction complexity not fully represented. 
  * For this range of values, the number of terminal nodes ranges from `r min(unique(xgb_hypergrid$max_depth))+1` to `r max(unique(xgb_hypergrid$max_depth))+1`

* **colsample_bytree** = `r unique(xgb_hypergrid$colsample_bytree)`
  * Fraction of features considered at each tree construction. A lower fraction  leads to less model variance, however, more bias as it may represent less complex interactions between features. 

* **subsample** = `r unique(xgb_hypergrid$subsample)`
  * Fraction of observations to be considered when building each tree. A lower fraction can reduce noise however it is less effective at preventing overfitting than colsample_bytree. 
  * Held fixed due to lower importance

* **nrounds** = `r unique(xgb_hypergrid$nrounds)` 
  * Number of trees in the model. Having less trees in the model will make it less complex and more robust against overfitting.Range of values selected in mind of computational power available. 
  * Near convergence of accuracy values attained at range maximum

```{r train_xgb, echo=FALSE}
# train model
set.seed(1235)
start_time <- Sys.time()
mod_xgb <- train(
        X_train, y_train,
        trControl = ctrl,
        tuneGrid = xgb_hypergrid,
        method = "xgbTree"
)
end_time <- Sys.time()
end_time - start_time
mod_xgb

#predict using processed test data
preds_xgb <- predict(object = mod_xgb, newdata = X_test)

#evaluate predictions using confusion matrix
cm_xgb = confusionMatrix(testing$classe, preds_xgb)
```

After tuning, the following XGBoost model is obtained:
```{r xgb_final, echo=FALSE, eval = TRUE, cache = TRUE}
df_tmp <- as.data.frame(mod_xgb$finalModel$params) %>% select(-objective, -validate_parameters)
xgb_final_params <- as.data.frame(t(as.matrix(df_tmp))) %>% rename(value = V1)
knitr::kable(xgb_final_params, caption = "Hyperparameters for final XGBoost Model")
```

Applying this model to the test data yields an overall accuracy of `r round(cm_xgb$overall[1],3)` and the following class-based results:
```{r xgb_cm, echo=FALSE, eval = TRUE, cache = TRUE}
knitr::kable(as.data.frame(cm_xgb$byClass) %>% select(Sensitivity,
                        Specificity, Precision, `Balanced Accuracy`) %>%
                     mutate(Sensitivity = round(Sensitivity,3),
                            Specificity = round(Specificity,3),
                            Precision = round(Precision,3),
                            `Balanced Accuracy` = round(`Balanced Accuracy`,3)))
```

### Random Forest

Random Forest is an ensemble method in which many weak learners (decision trees) are combined to create a more accurate aggregated model. In order to prevent overfitting, at each tree a bootstrapped sample of the data and at each node only a subset of the features are used (a node split will be based on a whichever feature split minimises the GINI/Entropy criteria). 
In this study, the "rf" method is used in the caret function. In this method, the following hyperparameter grid is tuned:
```{r hp_grid_rf, echo=FALSE}
# set out tuning hyperparameter grid - mtry (no. of sampled vars)
rf_hypergrid <- expand.grid(mtry=c(2,ceiling(sqrt(ncol(trainpca))),10,20,ncol(trainpca)));

# set range of ntrees
ntree_list <- c(10,20,50,100,200,250)
```

*  **mtry** = `r rf_hypergrid$mtry`
  * Number of features to consider at each node split. Less features reduces the model variance, however, the bias of an individual tree will increase as it will less represent complex interactions between features. Including a large number of features will increase the model train time. However, if there are a lot of noisy features, higher number increases chances of including quality features. Ideal number is generally the square root of total number of features (i.e. 5). As such, the grid includes a range of values around this number.
  
There is no option to tune the number of trees (**ntree**) in the caret train function, however, it will also be investigated in this study (with the caveat of no cross-validation):

* **ntree** = `r ntree_list`
  * Number of trees in random forest. Higher number creates a more robust aggregate model with less variance and lower model error (at the cost of training time). Random forests rarely overfit as such a high number of trees is acceptable. Main points to consider is if there are a lot of observations - a small number of trees means many observations may be left out. Range of of number of trees selected based on computational power limitations

```{r train_rf, echo=FALSE}
# Set-up for retaining each ntree model results
modellist <- list()
modeltraintime <- list()
col_names <- c("mtry","ntree","Accuracy","AccuracySD")
results_rf_df <- data.frame(matrix(nrow = 0, ncol = length(col_names)))
colnames(results_df) <- col_names
results_rf_df

start_time <- Sys.time()
for (ntree in ntree_list){
        set.seed(123)
        message("Random Forest, ntree = ", ntree)
        start_time1 <- Sys.time()
        fit <- train(Species ~.,
                     data = trainpca,
                     method = "rf",
                     tuneGrid = rf_hypergrid,
                     trControl = ctrl,
                     ntree = ntree)
        end_time1 <- Sys.time()
        key <- toString(ntree)
        modellist[[key]] <- fit
        modeltraintime[[key]] <- end_time1 - start_time1
        results_rf_df <- rbind(results_rf_df,
                            data.frame(mtry = fit$results$mtry,
                                       ntree = rep(ntree,nrow(rf_hypergrid)),
                                       Accuracy = fit$results$Accuracy,
                                       AccuracySD = fit$results$AccuracySD))
        print(results_rf_df)
}
end_time <- Sys.time()
end_time - start_time

# Select optimal model parameters from list
rf_opt_hp <- results_df %>% filter(Accuracy == max(Accuracy)) %>% filter(ntree == min(ntree))
rf_opt_hp

# Extract optimal model from list
mod_rf <- modellist[[as.character(rf_opt_hp$ntree)]]
mod_rf

#predict using processed test data
preds_rf <- predict(object = mod_rf, newdata = testpca)

#evaluate predictions using confusion matrix
cm_rf = confusionMatrix(testing$classe, preds_rf)
```

After tuning, the following Random Forest model is obtained:
```{r rf_final, echo=FALSE, eval = TRUE, cache = TRUE}
df_tmp <- data.frame(mtry = rf_opt_hp$mtry, ntree = rf_opt_hp$ntree)
rf_final_params <- as.data.frame(t(as.matrix(df_tmp))) %>% rename(value = V1)
knitr::kable(rf_final_params, caption = "Hyperparameters for final Random Forest Model")
```

Applying this model to the test data yields an overall accuracy of `r round(cm_rf$overall[1],3)` and the following class-based results:
```{r rf_cm, echo=FALSE, eval = TRUE, cache = TRUE}
knitr::kable(as.data.frame(cm_rf$byClass) %>% select(Sensitivity,
                        Specificity, Precision, `Balanced Accuracy`)%>%
                     mutate(Sensitivity = round(Sensitivity,3),
                            Specificity = round(Specificity,3),
                            Precision = round(Precision,3),
                            `Balanced Accuracy` = round(`Balanced Accuracy`,3)))
```

### k-Nearest Neighbour

k-Nearest Neighbour is an algorithm that classifies observations by assuming that observations in close proximity (in feature space) will have a similar class label. A common measurement of proximity is the Euclidean distance. To classify a particular observation, the K-NN algorithm can take the "k" nearest neighbours and do a majority vote, in which the most frequent class of the surrounding neighbours is assigned to the input observation.

In this study, the "knn" method is used in the caret function. In this method, the following hyperparameter grid is tuned:
```{r hp_grid_knn, echo=FALSE, cache = TRUE}
#hyperparameter grid - number of nearest neighbours to consider
knn_hypergrid <- expand.grid(k = c(1,2,3,5,8,10,15,25,40));
```
* **k** = `r knn_hypergrid$k`
  * Number of nearest neighbours. As k decreases (to 1) the predictions become less stable and more susceptible to noise in the data. As k increases, predictions become more stable due to majority voting however, the model accuracy will start to decrease once k is too large.

```{r train_knn, echo=FALSE, cache = TRUE}
# train model
set.seed(1235)
start_time <- Sys.time()
mod_knn <- train(classe ~ ., 
                 data = trainpca, 
                 method = "knn",
                 trControl = ctrl,
                 tuneGrid = knn_hypergrid
)#
mod_knn
end_time <- Sys.time()
end_time - start_time

#predict using processed test data
preds_knn <- predict(object = mod_knn, newdata = testpca)

#evaluate predictions using confusion matrix
cm_knn = confusionMatrix(testing$classe, preds_knn)
```

After tuning, the following k-Nearest Neighbour model is obtained:
```{r knn_final, echo=FALSE, eval = TRUE, cache = TRUE}
df_tmp <- data.frame(k = mod_knn$bestTune$k)
knn_final_params <- as.data.frame(t(as.matrix(df_tmp))) %>% rename(value = V1)
knitr::kable(knn_final_params, caption = "Hyperparameters for final k-Nearest Neighbours Model")
```

Applying this model to the test data yields an overall accuracy of `r round(cm_knn$overall[1],3)` and the following class-based results:
```{r knn_cm, echo=FALSE, eval = TRUE, cache = TRUE}
knitr::kable(as.data.frame(cm_knn$byClass) %>% select(Sensitivity,
                        Specificity, Precision, `Balanced Accuracy`)%>%
                     mutate(Sensitivity = round(Sensitivity,3),
                            Specificity = round(Specificity,3),
                            Precision = round(Precision,3),
                            `Balanced Accuracy` = round(`Balanced Accuracy`,3)))
```

### Model Tuning Results

The overall accuracy is generally high, with all models being >0.97. Accuracy by class is also high for each class, the highest accuracy tending to be either Class A or E for all models (>0.992 for both classes). A high accuracy for Class A was expected as it is the dominant class within the training dataset. For all models, Class C classification is the worst performing, ranging from 0.953 to 0.983. 

**XGBoost Tuning** : Accuracy of models appears to increase with number of trees (ntree). For a maximum of splits of 8 and 12 (max_depth), an asymptote in accuracy appears to be developing at higher tree numbers. No clear trend in accuracy is observed for the fraction of features considered at each tree (colsample_bytree).

**Random Forest Tuning**: At each respective number of features considered at each split the accuracy appears to be approaching an asymptote at higher number of trees. Uncertainty (size of error bars) also reduces as number of trees increases. Accuracy also appears to increase as the number of features considered at each split decreases.

**k-Nearest Neighbour Tuning**: Accuracy appears to decrease as the number of nearest neighbours increases. Also, the accuracy uncertainty appears to increase with k.

```{r plot_tuning_results, echo=FALSE, eval = TRUE, cache = TRUE, fig.align='center', fig.height=5, fig.width=12, fig.cap="**Figure 2**: Plots showing the accuracy results from the cross-validated hyperparameter tuning of the XGBoost (left), Random Forest (centre) and k-Nearest Neighbours (right) models. Error bars indicate the 95% confidence interval - assuming student-t distribution of the cross-validated results (n=30, degrees of freedom = 29)."}

#XGB uncertainty
results_xgb_df <- mod_xgb$results;
results_xgb_df$max_depth<- as.factor(results_xgb_df$max_depth)
results_xgb_df$colsample_bytree <-as.factor(results_xgb_df$colsample_bytree)
plot_xgb<- ggplot(results_xgb_df, aes(x=nrounds, y=Accuracy, group = max_depth, color=max_depth)) + 
        geom_errorbar(aes(ymin=Accuracy-zctrl_95*AccuracySD/sqrt(nctrl), 
                          ymax=Accuracy+zctrl_95*AccuracySD/sqrt(nctrl)), width=3) +
        geom_line() +
        geom_point()        +
        facet_grid(rows = vars(colsample_bytree), labeller = label_both, 
                   scales = "free", space = "free")  +      
        theme_bw() +
        theme(axis.line = element_line(colour = "black"),
              panel.background = element_blank())
#random forest uncertainty
results_rf_df$ntree <- as.factor(results_rf_df$ntree)
results_rf_df$mtry <- as.factor(results_rf_df$mtry)
plot_rf<- ggplot(results_rf_df, aes(x=ntree, y=Accuracy, group = mtry, color=mtry)) + 
        geom_errorbar(aes(ymin=Accuracy-zctrl_95*AccuracySD/sqrt(nctrl), 
                          ymax=Accuracy+zctrl_95*AccuracySD/sqrt(nctrl)), width=0.1) +
        geom_line() +
        geom_point()  +      
        theme_bw() +
        theme(axis.line = element_line(colour = "black"),
              panel.background = element_blank())
#knn uncertainty
plot_knn<- ggplot(mod_knn$results, aes(x=k, y=Accuracy)) + 
        geom_errorbar(aes(ymin=Accuracy-zctrl_95*AccuracySD/sqrt(nctrl), 
                          ymax=Accuracy+zctrl_95*AccuracySD/sqrt(nctrl)), width=0.1) +
        geom_line() +
        geom_point()  +      
        theme_bw() +
        theme(axis.line = element_line(colour = "black"),
              panel.background = element_blank())
# Plot uncertainty for all models
ggarrange(plot_xgb, plot_rf, plot_knn, ncol = 3)
```

## Model Ensembling

Model ensembling involves combining the predictions of multiple distinct model in order to obtain a higher accuracy with the aggregated result. In this section, we combine the test set predictions for the XGBoost, Random Forest and K-Nearest Neighbor models to create a model ensemble. In this study we use two methods:

a. **Majority Voting**
  - In majority voting for classification problems, an odd number of models are compared and for each observation, the modal class across the model predictions is selected as the final class label.

b. **Holdout Model Stacking**
  - In Holdout Stacking (or "Blending"), the test set predictions for the base learner models are combined to generate the training data for a metalearning algorithm. In this study, a Random Forest model is used as the metalearning algorithm.

```{r comb_preds, echo=FALSE}
#combine test data predictions from base learners
preds_df <- data.frame(preds_xgb, preds_rf, preds_knn, classe = testing$classe)
```

```{r maj_vote, echo=FALSE, eval=TRUE, cache = TRUE}
#mode function
Mode <- function(x) {
        ux <- unique(x)
        ux[which.max(tabulate(match(x, ux)))]
}
# apply majority vote
preds_maj_test <- as.factor(apply(preds_df[,-4],1,Mode))
# evaluate accuracy on test dataset
cm_summary <- function(preds,obs,model_name){
        cm <- as.data.frame(t(as.matrix(confusionMatrix(obs, preds)$overall))) %>% 
                mutate(Model = model_name) %>% select(Model,Accuracy,AccuracyLower,AccuracyUpper)
        return(cm)
}
cm_maj_test <- cm_summary(preds_maj_test,testing$classe,"Majority Vote")
```

```{r model_stack, echo=FALSE, cache = FALSE}
#train ensemble model (of base learner predictions) using random forest
mod_stack <- train(classe ~.,
             data = preds_df,
             method = "rf",
             tuneLength = 5,
             trControl = ctrl,
             ntree = 50)
mod_stack
```

```{r test_accuracy, echo=FALSE, eval = TRUE, cache = TRUE}
#evaluate model stack accuracy - 95% Confidence Intervals - assumed student-t distribution
accuracy_mod_stack <- mod_stack$results %>% filter(Accuracy == (max(Accuracy))) %>%
        mutate(AccuracyLower = Accuracy - zctrl_95*AccuracySD/sqrt(nctrl),
               AccuracyUpper = Accuracy + zctrl_95*AccuracySD/sqrt(nctrl)) %>% 
        mutate(Model = "Model Stack") %>% select(Model,Accuracy,AccuracyLower,AccuracyUpper)
# Base learner accuracy
cm_xgb_test <- cm_summary(preds_xgb,testing$classe,"XGBoost")
cm_rf_test <- cm_summary(preds_rf,testing$classe,"Random Forest")
cm_knn_test <- cm_summary(preds_knn,testing$classe,"k-NN")
```

Below is a comparison of the accuracy (and uncertainty) across models on the test set data:
```{r test_accuracy_out, echo=FALSE, eval=TRUE, cache = TRUE}
#model accuracy summary
summary_test <- rbind(cm_xgb_test,cm_rf_test,cm_knn_test,cm_maj_test,accuracy_mod_stack)
knitr::kable(summary_test %>% mutate(Accuracy = round(Accuracy,3), 
                                     AccuracyLower = round(AccuracyLower,3),
                                     AccuracyUpper = round(AccuracyUpper,3))
                     , caption = "**Table 2**: Summary of Accuracy on different model types - based on test dataset predictions. AccuracyLower/AccuracyUpper refer to the 95% confidence interval of the Accuracy")
```

## Model Evaluation

As the model stack was trained on the test data predictions, the actual model accuracy must be computed on the unseen validation data set. Below is a comparison of the accuracy (and uncertainty) across models on the validation dataset:
```{r final_accuracy, echo=FALSE}
#generate base learner predictions on the validation set
predsv_xgb <- predict(object = mod_xgb, newdata = X_validation)
predsv_rf <- predict(object = mod_rf, newdata = validationpca)
predsv_knn <- predict(object = mod_knn, newdata = validationpca)
#combine base learner validation set predictions
predsv_df <- data.frame(preds_xgb = predsv_xgb, 
                        preds_rf = predsv_rf, 
                        preds_knn = predsv_knn, 
                        classe = validation$classe)
#majority vote predictions
predsv_maj <- as.factor(apply(predsv_df[,-4],1,Mode))
#Feed ensemble learner with base learner predictions
predsv_stack <- predict(mod_stack,predsv_df)
```

```{r final_accuracy_table, echo=FALSE, eval = TRUE, cache = TRUE}
#Evaluate accuracy on all models
cm_xgb_final <- cm_summary(predsv_xgb,validation$classe,"XGBoost")
cm_rf_final <- cm_summary(predsv_rf,validation$classe,"Random Forest")
cm_knn_final <- cm_summary(predsv_knn,validation$classe,"k-NN")
cm_maj_final <- cm_summary(predsv_maj,validation$classe,"Majority Vote")
cm_stack_final <- cm_summary(predsv_stack,validation$classe,"Model Stack")
summary_final <- rbind(cm_xgb_final,cm_rf_final,cm_knn_final,cm_maj_final,cm_stack_final)
knitr::kable(summary_final %>% mutate(Accuracy = round(Accuracy,3), 
                                     AccuracyLower = round(AccuracyLower,3),
                                     AccuracyUpper = round(AccuracyUpper,3)), caption = "**Table 3**: Summary of Accuracy on different model types - based on validation dataset predictions. AccuracyLower/AccuracyUpper refer to the 95% confidence interval of the Accuracy")
```

From the model accuracies on the validation dataset, it is apparent that the k-Nearest Neighbours model has the highest estimated accuracy out of the base learners - its 95% confidence intervals are above both the range of the XGBoost and Random Forest models. 

From the tuning plots in figure 2, it is clear that the final XGBoost and k-Nearest Neighbour models do not have the optimum hyperparameter configuration as the accuracy has not yet plateaued for the maximum number of trees modelled. The limitation in number of trees was strictly due to computational constraints. Ideally, the number of trees would have been increased for both models until an asymptote was observed in the accuracy values. 

Overall, despite the k-Nearest Neighbour model having the highest estimated accuracy, the confidence intervals still overlap with the majority vote and model stack results, as such, it cannot be confirmed that the k-Nearest Neighbour model is statistically the best (at a 95% confidence level). Moreover, the final k-Nearest Neighbour model was based on k=1 (i.e. only one nearest neighbour) which will be sensitive to distortions such as noise, outliers, mislabelling of data etc. As such, the model stack will be taken as the final model to ensure the final overall model is robust against these issues (due to the inclusion of the XGBoost and Random Forest models).

```{r plot_final, echo=FALSE, eval = TRUE, cache = TRUE, fig.align='center', fig.height=5, fig.width=12, fig.cap="**Figure 3**: Model accuracies based on validation dataset predictions"}
#Plot accuracy
label <- c("Estimated Accuracy", "Lower Bound (95% CI)" , 
           "Upper Bound (95% CI)")
plot_tbl <- reshape2::melt(summary_final,"Model")
plot_summary<- ggplot(plot_tbl, aes(x=value, y=Model)) + 
        geom_path(aes(x=value, y=Model)) +
        geom_point(aes(x=value, y=Model, group = variable, color=variable), size=4) +
        scale_colour_discrete(name = "Accuracy Type", labels = label)  +      
        labs(x = "Accuracy", y = "Model Type") +
        theme_bw() +
        theme(axis.line = element_line(colour = "black"),
              panel.background = element_blank()) 
print(plot_summary)
```
